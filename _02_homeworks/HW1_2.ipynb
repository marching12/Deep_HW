{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e61b4d616c4cde13"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import imageio.v2 as imageio\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "img_arr = imageio.imread(os.path.join(os.path.pardir, \"_00_data\", \"a_image-dog\", \"bobby.jpg\")) #파일열기\n",
    "\n",
    "print(type(img_arr))  # 이미지 배열의 데이터 타입 출력\n",
    "print(img_arr.shape)  # 이미지의 형태 출력\n",
    "print(img_arr.dtype)  # 이미지의 데이터 타입 출력\n",
    "\n",
    "img = torch.from_numpy(img_arr) # numpy 배열을 PyTorch 텐서로 변환\n",
    "out = img.permute(2, 0, 1)  # 이미지의 차원을 (높이, 너비, 채널)에서 (채널, 높이, 너비)로 변환\n",
    "print(out.shape)  \n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "data_dir = os.path.join(os.path.pardir, \"_00_data\", \"b_image-cats\")\n",
    "\n",
    "# 해당 디렉토리 내에 있는 .png 확장자의 파일명을 가져옴\n",
    "filenames = [\n",
    "  name for name in os.listdir(data_dir) if os.path.splitext(name)[-1] == '.png'\n",
    "]\n",
    "print(filenames)\n",
    "\n",
    "\n",
    "# 각 파일을 열어서 이미지 출력 및 정보 출력\n",
    "for i, filename in enumerate(filenames):\n",
    "  image = Image.open(os.path.join(data_dir, filename))\n",
    "  image.show()\n",
    "  img_arr = imageio.imread(os.path.join(data_dir, filename))\n",
    "  print(img_arr.shape)\n",
    "  print(img_arr.dtype)\n",
    "\n",
    "batch_size = 3  # 배치 사이즈 설정 \n",
    "batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8) # (배치, 채널, 높이, 너비)의 빈 텐서 생성\n",
    "\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "  img_arr = imageio.imread(os.path.join(data_dir, filename))\n",
    "  img_t = torch.from_numpy(img_arr)\n",
    "  img_t = img_t.permute(2, 0, 1)\n",
    "  batch[i] = img_t  # 변환된 텐서를 배치에 추가\n",
    "\n",
    "print(batch.shape)  # 배치 텐서의 크기 출력\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "batch = batch.float()\n",
    "batch /= 255.0  #이미지값을 0~1로 정규화\n",
    "print(batch.dtype)\n",
    "print(batch.shape)\n",
    "\n",
    "n_channels = batch.shape[1]\n",
    "\n",
    "for c in range(n_channels):\n",
    "  mean = torch.mean(batch[:, c])\n",
    "  std = torch.std(batch[:, c])\n",
    "  print(mean, std)\n",
    "  batch[:, c] = (batch[:, c] - mean) / std"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# A 고찰내용\n",
    "#### 이미지 데이터 확인 및 출력\n",
    "1. 이미지 파일을 텐서로 관리할 수 있다.\n",
    "2. 텐서 차원의 순서를 변경할 수 있다.\n",
    "3. 정규화를 할 수 있다."
   ],
   "id": "3efe0a1640469178"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "# DICOM 볼륨 데이터가 있는 디렉토리 경로 설정\n",
    "dir_path = os.path.join(os.path.pardir, \"_00_data\", \"c_volumetric-dicom\", \"2-LUNG_3.0_B70f-04083\")  # 파일 경로 코드 수정\n",
    "# DICOM 볼륨 데이터를 읽어 numpy 배열로 변환\n",
    "vol_array = imageio.volread(dir_path, format='DICOM')\n",
    "print(type(vol_array))   # >>> <class 'imageio.core.util.Array'>:  Numpy NDArray\n",
    "# 이는 99개의 슬라이스(slice)로 구성된 512x512 크기의 3D 이미지임을 의미함\n",
    "print(vol_array.shape)   # >>> (99, 512, 512)\n",
    "# vol_array의 데이터 타입 출력: DICOM 이미지 파일에서 추출된 int16 데이터 타입\n",
    "print(vol_array.dtype)   # >>> int16\n",
    "print(vol_array[0])\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for id in range(0, 99):\n",
    "  fig.add_subplot(10, 10, id + 1)\n",
    "  plt.imshow(vol_array[id])\n",
    "plt.show()  # 전체 슬라이스 이미지들을 한 번에 표시\n",
    "\n",
    "import torch\n",
    "# numpy 배열을 PyTorch 텐서로 변환하고, float 타입으로 변환\n",
    "vol = torch.from_numpy(vol_array).float()\n",
    "# 첫 번째 차원에 채널 차원을 추가 (예: [C, D, H, W] 형태로 만듦)\n",
    "vol = torch.unsqueeze(vol, 0)  # channel\n",
    "# 두 번째 차원에 데이터 차원을 추가하여 [B, C, D, H, W] 형태로 만듦\n",
    "vol = torch.unsqueeze(vol, 0)  # data size\n",
    "\n",
    "print(vol.shape)  # >>> torch.Size([1, 1, 99, 512, 512])\n",
    "  \n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "mean = torch.mean(vol, dim=(3, 4), keepdim=True)  # mean over all of dim=(3, 4)\n",
    "print(mean.shape)\n",
    "std = torch.std(vol, dim=(3, 4), keepdim=True)    # std over all of dim=(3, 4)\n",
    "print(std.shape)\n",
    "vol = (vol - mean) / std\n",
    "print(vol.shape)\n",
    "\n",
    "print(vol[0, 0, 0])  "
   ],
   "id": "60a1df99f1ba0926",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# B 고찰내용\n",
    "1. DICOM 데이터의 읽기 및 numpy 배열 변환을 할 수 있다.\n",
    "2. 데이터 형태 및 타입 확인할 수 있다.\n",
    "3. 이미지 시각화를 할 수 있다.\n",
    "4. numpy를 텐서로 변환할 수 있다.\n",
    "5. 정규화를 할 수 있다.\n",
    "6. 3D 볼륨 데이터에서 특정 슬라이스에 접근할 수 있다."
   ],
   "id": "627ce1ade69f5125"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "wine_path = os.path.join(os.path.pardir, \"_00_data\", \"d_tabular-wine\", \"winequality-white.csv\")\n",
    "# 데이터를 NumPy 배열로 로드 (헤더를 건너뛰고, ';'로 구분된 값)\n",
    "wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\", skiprows=1)\n",
    "print(wineq_numpy.dtype)\n",
    "print(wineq_numpy.shape)\n",
    "print(wineq_numpy)\n",
    "print()\n",
    "# CSV 파일에서 컬럼 목록 읽기\n",
    "col_list = next(csv.reader(open(wine_path), delimiter=';'))\n",
    "print(col_list)\n",
    "print()\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "import torch\n",
    "# NumPy 배열을 PyTorch 텐서로 변환\n",
    "wineq = torch.from_numpy(wineq_numpy)\n",
    "print(wineq.dtype)\n",
    "print(wineq.shape)\n",
    "print()\n",
    "\n",
    "# 특성과 타겟을 분리\n",
    "data = wineq[:, :-1]  # Selects all rows and all columns except the last\n",
    "print(data.dtype)\n",
    "print(data.shape)\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "target = wineq[:, -1]  # Selects all rows and the last column\n",
    "print(target.dtype)\n",
    "print(target.shape)\n",
    "print(target)\n",
    "print()\n",
    "\n",
    "target = target.long()  # treat labels as an integer\n",
    "print(target.dtype)\n",
    "print(target.shape)\n",
    "print(target)\n",
    "print()\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# 원-핫 인코딩을 위한 단위 행렬 생성\n",
    "eye_matrix = torch.eye(10)\n",
    "# We use the 'target' tensor as indices to extract the corresponding rows from the identity matrix\n",
    "# It can generate the one-hot vectors for each element in the 'target' tensor\n",
    "onehot_target = eye_matrix[target]\n",
    "\n",
    "print(onehot_target.shape)  # >>> torch.Size([4898, 10])\n",
    "print(onehot_target[0])\n",
    "print(onehot_target[1])\n",
    "print(onehot_target[-2])\n",
    "print(onehot_target)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "# 데이터 정규화\n",
    "data_mean = torch.mean(data, dim=0)\n",
    "data_var = torch.var(data, dim=0)\n",
    "data = (data - data_mean) / torch.sqrt(data_var)\n",
    "print(data)\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 데이터 분할: 훈련 세트와 검증 세트\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, onehot_target, test_size=0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "def get_wine_data():  #함수화\n",
    "  # 데이터 로드 및 전처리 함수 정의\n",
    "  wine_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"d_tabular-wine\", \"winequality-white.csv\")\n",
    "  wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\", skiprows=1)\n",
    "  \n",
    "  # NumPy 배열을 PyTorch 텐서로 변환\n",
    "  wineq = torch.from_numpy(wineq_numpy)\n",
    "  # 특성과 타겟을 분리\n",
    "  data = wineq[:, :-1]  # Selects all rows and all columns except the last\n",
    "  target = wineq[:, -1].long()  # treat labels as an integer\n",
    "  # 원-핫 인코딩\n",
    "  eye_matrix = torch.eye(10)\n",
    "  onehot_target = eye_matrix[target]\n",
    "  # 데이터 정규화\n",
    "  data_mean = torch.mean(data, dim=0)\n",
    "  data_var = torch.var(data, dim=0)\n",
    "  data = (data - data_mean) / torch.sqrt(data_var)\n",
    "\n",
    "  X_train, X_valid, y_train, y_valid = train_test_split(data, onehot_target, test_size=0.2)\n",
    "\n",
    "  return X_train, X_valid, y_train, y_valid"
   ],
   "id": "56b5f0abf8afbd38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# C 고찰내용\n",
    "1. csv 파일을 읽고 로드할 수 있다.\n",
    "2. 타겟을 정수형으로 변환하고 원 핫 인코딩 후 정규화할 수 있다.\n",
    "* 원 핫 인코딩은 보통 범주형 데이터를 수치형으로 바꿀 때 사용한다.\n",
    "3. 데이터분할이 가능하다.\n",
    "4. 함수화하여 사용 가능하다."
   ],
   "id": "33e2d1390166ea4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# https://medium.com/analytics-vidhya/implement-linear-regression-on-boston-housing-dataset-by-pytorch-c5d29546f938\n",
    "# https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset\n",
    "import torch\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "# California Housing 데이터셋을 로드합니다.\n",
    "housing = fetch_california_housing()\n",
    "print(housing.keys()) # 데이터셋의 키를 출력하여 어떤 정보가 포함되어 있는지 확인합니다.\n",
    "\n",
    "# 데이터와 관련된 정보 출력\n",
    "print(type(housing.data))  # 데이터 타입 확인 (일반적으로 <class 'numpy.ndarray'>)\n",
    "print(housing.data.dtype)  # 데이터의 타입 확인 (예: float64)\n",
    "print(housing.data.shape)  # 데이터 배열의 형태 확인 (예: (20640, 8))\n",
    "print(housing.feature_names)  # 데이터의 특성 이름 출력\n",
    "\n",
    "print(housing.target.shape)  # 타겟 배열의 형태 확인 (예: (20640,))\n",
    "print(housing.target_names)  # 타겟의 이름 출력 (California Housing 데이터는 타겟 이름이 없음)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(housing.data.min(), housing.data.max())\n",
    "#정규화\n",
    "data_mean = np.mean(housing.data, axis=0)\n",
    "data_var = np.var(housing.data, axis=0)\n",
    "data = (housing.data - data_mean) / np.sqrt(data_var)\n",
    "target = housing.target\n",
    "\n",
    "\n",
    "print(data.min(), data.max()) # 정규화된 데이터 최소값과 최대값을 출력하여 정규화 확인.\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 데이터를 훈련 세트와 테스트 세트로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n",
    "\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터의 형태를 출력하여 확인합니다.\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ],
   "id": "83d90860ded12bcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# D 고찰내용\n",
    "1. fetch_california_housing() 함수를 통해 California Housing 데이터셋을 로드할 수 있다.\n",
    "2. 정규화의 필요성\n",
    "* 데이터의 범위가 서로 다를 경우, 모델학습의 성능을 향상시키기 위해 정규화가 필요하다.\n",
    "* 방법 : 평균과 분산계산 ->  정규화 적용 -> 정규화 확인\n",
    "3. 훈련 및 테스트 데이터로 분할 한다.\n",
    "* 모델의 일반화 성능 평가와 과적합 방지를 위해 분할한다. \n",
    "4. 훈련데이터와 테스트 데이터의 형태를 출력하여 검사할 필요가 있다."
   ],
   "id": "f1fbbd17a7ce6d34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.set_printoptions(edgeitems=2, threshold=50, linewidth=75) # PyTorch의 출력 옵션 설정: 배열의 끝부분을 2개 항목만 표시하고, 전체 출력 개수 제한\n",
    "\n",
    "bikes_path = os.path.join(os.path.pardir, \"_00_data\", \"e_time-series-bike-sharing-dataset\", \"hour-fixed.csv\")\n",
    "\n",
    "bikes_numpy = np.loadtxt(   # CSV 파일에서 데이터를 NumPy 배열로 로드\n",
    "  fname=bikes_path, dtype=np.float32, delimiter=\",\", skiprows=1,\n",
    "  converters={\n",
    "    1: lambda x: float(x[8:10])  # 2011-01-07 --> 07 --> 7.0\n",
    "  }\n",
    ")\n",
    "bikes = torch.from_numpy(bikes_numpy)\n",
    "print(bikes.shape)\n",
    "\n",
    "# 데이터를 하루 단위로 재구성 (730일, 24시간, 17개의 특성)\n",
    "daily_bikes = bikes.view(-1, 24, bikes.shape[1])\n",
    "print(daily_bikes.shape)  # >>> torch.Size([730, 24, 17])\n",
    "# 특성과 타겟을 분리\n",
    "daily_bikes_data = daily_bikes[:, :, :-1]\n",
    "daily_bikes_target = daily_bikes[:, :, -1].unsqueeze(dim=-1)\n",
    "\n",
    "print(daily_bikes_data.shape)\n",
    "print(daily_bikes_target.shape)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "# 첫 번째 날의 데이터 추출\n",
    "first_day_data = daily_bikes_data[0]\n",
    "print(first_day_data.shape)\n",
    "\n",
    "# 날씨 상황 열을 정수형으로 출력\n",
    "print(first_day_data[:, 9].long())\n",
    "# 날씨 상황을 원-핫 인코딩하기 위한 단위 행렬 생성\n",
    "eye_matrix = torch.eye(4)\n",
    "print(eye_matrix)\n",
    "\n",
    "# 날씨 상황 열을 원-핫 인코딩\n",
    "weather_onehot = eye_matrix[first_day_data[:, 9].long() - 1]\n",
    "print(weather_onehot.shape)\n",
    "print(weather_onehot)\n",
    "\n",
    "# 원-핫 인코딩된 날씨 데이터를 기존의 데이터와 연결\n",
    "first_day_data_torch = torch.cat(tensors=(first_day_data, weather_onehot), dim=1)\n",
    "print(first_day_data_torch.shape)\n",
    "print(first_day_data_torch)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# 모든 날에 대해 날씨 상황을 원-핫 인코딩하고, 기존 데이터와 연결\n",
    "day_data_torch_list = []\n",
    "\n",
    "for daily_idx in range(daily_bikes_data.shape[0]):  # range(730)\n",
    "  day = daily_bikes_data[daily_idx]  # day.shape: [24, 16]\n",
    "  weather_onehot = eye_matrix[day[:, 9].long() - 1]\n",
    "  day_data_torch = torch.cat(tensors=(day, weather_onehot), dim=1)  # day_data_torch.shape: [24, 20]\n",
    "  day_data_torch_list.append(day_data_torch)\n",
    "\n",
    "print(len(day_data_torch_list))\n",
    "daily_bikes_data = torch.stack(day_data_torch_list, dim=0)\n",
    "print(daily_bikes_data.shape)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "# 데이터에서 'instant'와 'wheathersit' 열을 제거하고 나머지 열을 사용\n",
    "print(daily_bikes_data[:, :, :9].shape, daily_bikes_data[:, :, 10:].shape)\n",
    "daily_bikes_data = torch.cat(\n",
    "  [daily_bikes_data[:, :, 1:9], daily_bikes_data[:, :, 10:]], dim=2\n",
    ") # Drop 'instant' and 'whethersit' columns\n",
    "print(daily_bikes_data.shape)\n",
    "\n",
    "# 온도 열을 정규화 (평균이 0, 표준편차가 1이 되도록 변환)\n",
    "temperatures = daily_bikes_data[:, :, 8]\n",
    "daily_bikes_data[:, :, 8] = (daily_bikes_data[:, :, 8] - torch.mean(temperatures)) / torch.std(temperatures)\n"
   ],
   "id": "6cd52369fe991a9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# E 고찰 내용\n",
    "1. 데이터 재구성\n",
    "* bikes.view(-1, 24, bikes.shape[1])로 데이터셋을 하루단위로 재구성할 수 있다.\n",
    "* 특성과 타겟을 분리하고 나머지 열을 특성으로 사용가능하다.\n",
    "2. 원 핫 인코딩\n",
    "* torch.eye(4)를 통해 단위 행렬을 생성할 수 있다.\n",
    "* eye_matrix[first_day_data[:, 9].long() - 1]를 통해 날씨 상황 열을 원-핫 인코딩하여 범주형 변수를 수치형 벡터로 변환할 수 있다.\n",
    "* torch.cat()로 기존 데이터와 원 핫 인코딩된 데이터를 합칠 수 있다.\n",
    "3. 데이터 정리 및 처리\n",
    "* 모든 날의 데이터를 리스트로 저장하고, torch.stack를 사용하여 텐서로 변환할 수 있다.\n",
    "* torch.cat를 사용하여 불필요한 열을 제거할 수 있다."
   ],
   "id": "832628d2e471f3d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_PATH = str(Path(\"Deep_HW\").resolve().parent.parent) # 파일경로 수정\n",
    "import sys\n",
    "# BASE_PATH를 시스템 경로에 추가합니다.\n",
    "# 이렇게 하면 BASE_PATH에 있는 모듈들을 임포트할 수 있다.\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "torch.set_printoptions(edgeitems=2, threshold=50, linewidth=75)\n",
    "\n",
    "bikes_path = os.path.join(BASE_PATH, \"_00_data\", \"e_time-series-bike-sharing-dataset\", \"hour-fixed.csv\")\n",
    "\n",
    "bikes_numpy = np.loadtxt(\n",
    "  fname=bikes_path, dtype=np.float32, delimiter=\",\", skiprows=1,\n",
    "  converters={\n",
    "    1: lambda x: float(x[8:10])  # 2011-01-07 --> 07 --> 7\n",
    "  }\n",
    ")\n",
    "\n",
    "bikes_data = torch.from_numpy(bikes_numpy).to(torch.float)\n",
    "print(bikes_data.shape)    # 텐서의 형태 출력; torch.Size([17520, 17])\n",
    "bikes_target = bikes_data[:, -1].unsqueeze(dim=-1)  # 'cnt' 열을 타겟으로 설정\n",
    "bikes_data = bikes_data[:, :-1]   # 마지막 열 'cnt'를 제외한 나머지 데이터 사용; torch.Size([17520, 16])\n",
    "\n",
    "# 날씨 정보를 원-핫 인코딩하기 위한 단위 행렬 생성\n",
    "eye_matrix = torch.eye(4)\n",
    "\n",
    "# 데이터를 처리하여 원-핫 인코딩을 적용\n",
    "data_torch_list = []\n",
    "for idx in range(bikes_data.shape[0]):  # range(730)\n",
    "  hour_data = bikes_data[idx]  # 시간 단위 데이터 추출\n",
    "  weather_onehot = eye_matrix[hour_data[9].long() - 1]\n",
    "  concat_data_torch = torch.cat(tensors=(hour_data, weather_onehot), dim=-1)\n",
    "  # concat_data_torch.shape: [20]\n",
    "  data_torch_list.append(concat_data_torch)\n",
    "\n",
    "# 처리된 데이터를 하나의 텐서로 변환하고, 필요 없는 열을 제거\n",
    "bikes_data = torch.stack(data_torch_list, dim=0)\n",
    "bikes_data = torch.cat([bikes_data[:, 1:9], bikes_data[:, 10:]], dim=-1)\n",
    "# Drop 'instant' and 'whethersit' columns\n",
    "\n",
    "print(bikes_data.shape)\n",
    "print(bikes_data[0])\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "# 시퀀스 크기, 검증 크기, 테스트 크기 설정\n",
    "sequence_size = 24\n",
    "validation_size = 96\n",
    "test_size = 24\n",
    "y_normalizer = 100\n",
    "\n",
    "# 전체 데이터 크기와 훈련, 검증, 테스트 크기 계산\n",
    "data_size = len(bikes_data) - sequence_size + 1\n",
    "print(\"data_size: {0}\".format(data_size))\n",
    "train_size = data_size - (validation_size + test_size)\n",
    "print(\"train_size: {0}, validation_size: {1}, test_size: {2}\".format(train_size, validation_size, test_size))\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "#################################################################################################\n",
    "# 훈련 데이터를 생성\n",
    "row_cursor = 0\n",
    "\n",
    "X_train_list = []\n",
    "y_train_regression_list = []\n",
    "for idx in range(0, train_size):\n",
    "  sequence_data = bikes_data[idx: idx + sequence_size]  # 시퀀스 데이터를 추출\n",
    "  sequence_target = bikes_target[idx + sequence_size - 1] # 시퀀스의 마지막 값을 타겟으로 설정\n",
    "  X_train_list.append(sequence_data)\n",
    "  y_train_regression_list.append(sequence_target)\n",
    "  row_cursor += 1\n",
    "\n",
    "X_train = torch.stack(X_train_list, dim=0).to(torch.float)  # 훈련 데이터 텐서로 변환  \n",
    "print(X_train.shape)\n",
    "y_train_regression = torch.tensor(y_train_regression_list, dtype=torch.float32) / y_normalizer  # 타겟 데이터 정규화\n",
    "\n",
    "# 훈련 데이터 정규화\n",
    "m = X_train.mean(dim=0, keepdim=True)\n",
    "s = X_train.std(dim=0, keepdim=True)\n",
    "X_train = (X_train - m) / s\n",
    "\n",
    "print(X_train.shape, y_train_regression.shape)\n",
    "# >>> torch.Size([17376, 24, 19]) torch.Size([17376])\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "#################################################################################################\n",
    "# 검증 데이터를 생성\n",
    "X_validation_list = []\n",
    "y_validation_regression_list = []\n",
    "for idx in range(row_cursor, row_cursor + validation_size):\n",
    "  sequence_data = bikes_data[idx: idx + sequence_size]  # 시퀀스 데이터를 추출\n",
    "  sequence_target = bikes_target[idx + sequence_size - 1]\n",
    "  X_validation_list.append(sequence_data)\n",
    "  y_validation_regression_list.append(sequence_target)\n",
    "  row_cursor += 1\n",
    "\n",
    "X_validation = torch.stack(X_validation_list, dim=0).to(torch.float)\n",
    "y_validation_regression = torch.tensor(y_validation_regression_list, dtype=torch.float32) / y_normalizer\n",
    "\n",
    "X_validation = (X_validation - m) / s\n",
    "\n",
    "print(X_validation.shape, y_validation_regression.shape)\n",
    "# >>> torch.Size([96, 24, 19]) torch.Size([96])\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "#################################################################################################\n",
    "# 테스트 데이터를 생성\n",
    "X_test_list = []\n",
    "y_test_regression_list = []\n",
    "for idx in range(row_cursor, row_cursor + test_size):\n",
    "  sequence_data = bikes_data[idx: idx + sequence_size]  # 시퀀스 데이터를 추출\n",
    "  sequence_target = bikes_target[idx + sequence_size - 1]\n",
    "  X_test_list.append(sequence_data)\n",
    "  y_test_regression_list.append(sequence_target)\n",
    "  row_cursor += 1\n",
    "\n",
    "X_test = torch.stack(X_test_list, dim=0).to(torch.float)  # 테스트 데이터 텐서로 변환\n",
    "y_test_regression = torch.tensor(y_test_regression_list, dtype=torch.float32) / y_normalizer\n",
    "\n",
    "X_test -= (X_test - m) / s\n",
    "\n",
    "print(X_test.shape, y_test_regression.shape)\n",
    "# >>> torch.Size([24, 24, 18]) torch.Size([24])"
   ],
   "id": "6d5e5b862a8cf0ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# F 고찰내용\n",
    "1. 데이터 로딩 및 변환\n",
    "* NumPy와 PyTorch의 상호운용성: 데이터를 np.loadtxt로 불러오고, 이를 torch.from_numpy()를 사용하여 PyTorch 텐서로 변환할 수 있다.\n",
    "* 특정 열 처리: converters를 사용하여 데이터를 로드할 때 특정 열의 값을 변환할 수 있다.\n",
    "2. 슬라이딩 윈도우 기법\n",
    "* 시계열 데이터에서 시퀀스 생성: 각 시퀀스가 과거 sequence_size 동안의 데이터를 포함하고, 그에 따라 하나의 타겟 값(자전거 대여 수)를 예측할 수 있다.\n",
    "* 타겟과 시퀀스 데이터의 정렬: 시퀀스 데이터를 생성하고, 그 시퀀스의 마지막 값이 타겟 데이터로 설정되는 방식을 알 수 있다.\n",
    "4. 데이터 정규화\n",
    "* 훈련 데이터의 평균과 표준편차를 사용한 정규화: 훈련 데이터의 평균과 표준편차를 구해 이를 활용하여 훈련, 검증, 테스트 데이터를 정규화할 수 있다, 모델이 수렴하는 속도를 개선할 수 있고, 이상치에 민감하지 않게 할 수 있다.\n",
    "* 데이터셋 간의 일관된 정규화: 모델이 과적합(overfitting)하지 않게 하고 일관된 성능을 유지할 수 있다.\n",
    "5. 훈련, 검증, 테스트 데이터 분리\n",
    "* 데이터 분할: 훈련, 검증, 테스트 데이터를 나누는 과정을 통해, 모델이 새로운 데이터를 얼마나 잘 예측할 수 있는지 평가할 수 있다.\n",
    "* 슬라이딩 윈도우 기반 훈련 데이터 생성: 각 데이터셋에 대해 for 루프를 사용해 슬라이딩 윈도우 방식으로 데이터를 분리할 수 있다.\n",
    "7. 모델 훈련을 위한 데이터 준비\n",
    "* 시계열 데이터를 모델에 맞게 정렬: 시계열 데이터의 각 시간 단위 데이터를 하나의 샘플을 정규화하여 딥러닝 모델이 다룰 수 있는 형태로 바꿀 수 있다.\n"
   ],
   "id": "1c7367619a54676e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# BASE_PATH 설정: 프로젝트의 루트 경로를 기준으로 설정 (현재 파일의 상위 2개의 디렉토리)\n",
    "BASE_PATH = str(Path(\"Deep_HW\").resolve().parent.parent)    #코드 수정\n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "# BTC_KRW 데이터 경로 설정 및 CSV 파일 읽기\n",
    "btc_krw_path = os.path.join(BASE_PATH, \"_00_data\", \"k_cryptocurrency\", \"BTC_KRW.csv\")\n",
    "df = pd.read_csv(btc_krw_path)  # CSV 파일을 DataFrame으로 로드\n",
    "print(df)\n",
    "\n",
    "# 데이터의 총 행(row) 크기 출력\n",
    "row_size = len(df)\n",
    "print(\"row_size:\", row_size)\n",
    "\n",
    "# 데이터의 컬럼 출력 (['Date', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "columns = df.columns\n",
    "print([column for column in columns])\n",
    "\n",
    "# 'Date' 컬럼을 별도로 저장하고, DataFrame에서 제거\n",
    "date_list = df['Date']\n",
    "df = df.drop(columns=['Date'])\n",
    "\n",
    "print(df)\n",
    "print(\"#\" * 100, 0)\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "# 시퀀스 크기, 검증 데이터 크기, 테스트 데이터 크기 설정\n",
    "sequence_size = 10\n",
    "validation_size = 100\n",
    "test_size = 50\n",
    "\n",
    "# 데이터 크기 계산: (총 행 크기 - 시퀀스 크기 + 1)\n",
    "data_size = row_size - sequence_size + 1\n",
    "print(\"data_size: {0}\".format(data_size))\n",
    "\n",
    "# 훈련, 검증, 테스트 데이터 크기 설정\n",
    "train_size = data_size - (validation_size + test_size)\n",
    "print(\"train_size: {0}, validation_size: {1}, test_size: {2}\".format(train_size, validation_size, test_size))\n",
    "\n",
    "print(\"#\" * 100, 1)\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "# 훈련 데이터 준비\n",
    "row_cursor = 0  # 데이터를 순차적으로 처리하기 위한 커서\n",
    "y_normalizer = 1.0e7  # 가격 데이터를 정규화하기 위한 스케일링 값\n",
    "\n",
    "# 훈련 데이터를 담을 리스트 초기화\n",
    "X_train_list = []\n",
    "y_train_regression_list = []\n",
    "y_train_classification_list = []\n",
    "y_train_date = []\n",
    "\n",
    "# 훈련 데이터 생성 (시퀀스 기반)\n",
    "for idx in range(0, train_size):\n",
    "    # 시퀀스 데이터를 추출 (시퀀스 크기만큼의 데이터)\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 5)\n",
    "    X_train_list.append(torch.from_numpy(sequence_data))\n",
    "    \n",
    "    # 시퀀스의 마지막 값(Close)을 회귀 타겟으로 설정\n",
    "    y_train_regression_list.append(df.iloc[idx + sequence_size - 1][\"Close\"])\n",
    "    \n",
    "    # 시퀀스의 마지막 두 값(Close)을 비교하여 상승/하락 여부를 분류 타겟으로 설정\n",
    "    y_train_classification_list.append(\n",
    "        1 if df.iloc[idx + sequence_size - 1][\"Close\"] >= df.iloc[idx + sequence_size - 2][\"Close\"] else 0\n",
    "    )\n",
    "    \n",
    "    # 해당 시퀀스의 날짜 저장\n",
    "    y_train_date.append(date_list[idx + sequence_size - 1])\n",
    "    row_cursor += 1\n",
    "\n",
    "# 리스트 형태의 데이터를 텐서로 변환\n",
    "X_train = torch.stack(X_train_list, dim=0).to(torch.float)\n",
    "y_train_regression = torch.tensor(y_train_regression_list, dtype=torch.float32) / y_normalizer  # 정규화된 회귀 타겟\n",
    "y_train_classification = torch.tensor(y_train_classification_list, dtype=torch.int64)  # 분류 타겟\n",
    "print(y_train_classification)\n",
    "\n",
    "# 데이터 정규화 (평균, 표준편차)\n",
    "m = X_train.mean(dim=0, keepdim=True)\n",
    "s = X_train.std(dim=0, keepdim=True)\n",
    "X_train -= m\n",
    "X_train /= s\n",
    "print(X_train.shape, y_train_regression.shape, y_train_classification.shape)\n",
    "print(\"Label - Start Date: {0} ~ End Date: {1}\".format(y_train_date[0], y_train_date[-1]))\n",
    "\n",
    "print(\"#\" * 100, 2)\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "# 검증 데이터 준비\n",
    "X_validation_list = []\n",
    "y_validation_regression_list = []\n",
    "y_validation_classification_list = []\n",
    "y_validation_date = []\n",
    "\n",
    "# 검증 데이터 생성\n",
    "for idx in range(row_cursor, row_cursor + validation_size):\n",
    "    # 시퀀스 데이터를 추출\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values     # sequence_data.shape: (sequence_size, 5)\n",
    "    X_validation_list.append(torch.from_numpy(sequence_data))\n",
    "    \n",
    "    # 시퀀스의 마지막 값(Close)을 회귀 타겟으로 설정\n",
    "    y_validation_regression_list.append(df.iloc[idx + sequence_size - 1][\"Close\"])\n",
    "    \n",
    "    # 시퀀스의 마지막 두 값(Close)을 비교하여 상승/하락 여부를 분류 타겟으로 설정\n",
    "    y_validation_classification_list.append(\n",
    "        1 if df.iloc[idx + sequence_size - 1][\"Close\"] >= df.iloc[idx + sequence_size - 2][\"Close\"] else 0\n",
    "    )\n",
    "    \n",
    "    # 해당 시퀀스의 날짜 저장\n",
    "    y_validation_date.append(date_list[idx + sequence_size - 1])\n",
    "    row_cursor += 1\n",
    "\n",
    "# 리스트 데이터를 텐서로 변환\n",
    "X_validation = torch.stack(X_validation_list, dim=0).to(torch.float)\n",
    "y_validation_regression = torch.tensor(y_validation_regression_list, dtype=torch.float32) / y_normalizer  # 정규화된 회귀 타겟\n",
    "y_validation_classification = torch.tensor(y_validation_classification_list, dtype=torch.int64)  # 분류 타겟\n",
    "print(y_validation_classification)\n",
    "\n",
    "# 검증 데이터 정규화 (훈련 데이터에서 구한 평균, 표준편차 사용)\n",
    "X_validation = (X_validation - m) / s\n",
    "print(X_validation.shape, y_validation_regression.shape, y_validation_classification.shape)\n",
    "print(\"Label - Start Date: {0} ~ End Date: {1}\".format(y_validation_date[0], y_validation_date[-1]))\n",
    "\n",
    "print(\"#\" * 100, 3)\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "# 테스트 데이터 준비\n",
    "X_test_list = []\n",
    "y_test_regression_list = []\n",
    "y_test_classification_list = []\n",
    "y_test_date = []\n",
    "\n",
    "# 테스트 데이터 생성\n",
    "for idx in range(row_cursor, row_cursor + test_size):\n",
    "    # 시퀀스 데이터를 추출\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values   # sequence_data.shape: (sequence_size, 5)\n",
    "    X_test_list.append(torch.from_numpy(sequence_data))\n",
    "    \n",
    "    # 시퀀스의 마지막 값(Close)을 회귀 타겟으로 설정\n",
    "    y_test_regression_list.append(df.iloc[idx + sequence_size - 1][\"Close\"])\n",
    "    \n",
    "    # 시퀀스의 마지막 두 값(Close)을 비교하여 상승/하락 여부를 분류 타겟으로 설정\n",
    "    y_test_classification_list.append(\n",
    "        1 if df.iloc[idx + sequence_size - 1][\"Close\"] > df.iloc[idx + sequence_size - 2][\"Close\"] else 0\n",
    "    )\n",
    "    \n",
    "    # 해당 시퀀스의 날짜 저장\n",
    "    y_test_date.append(date_list[idx + sequence_size - 1])\n",
    "    row_cursor += 1\n",
    "\n",
    "# 리스트 데이터를 텐서로 변환\n",
    "X_test = torch.stack(X_test_list, dim=0).to(torch.float)\n",
    "y_test_regression = torch.tensor(y_test_regression_list, dtype=torch.float32) / y_normalizer  # 정규화된 회귀 타겟\n",
    "y_test_classification = torch.tensor(y_test_classification_list, dtype=torch.int64)  # 분류 타겟\n",
    "print(y_test_classification)\n",
    "\n",
    "# 테스트 데이터 정규화 (훈련 데이터에서 구한 평균, 표준편차 사용)\n",
    "X_test = (X_test - m) / s\n",
    "print(X_test.shape, y_test_regression.shape, y_test_classification.shape)\n",
    "print(\"Label - Start Date: {0} ~ End Date: {1}\".format(y_test_date[0], y_test_date[-1]))\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "# 그래프 생성\n",
    "fig, ax = plt.subplots(1, figsize=(13, 7))\n",
    "\n",
    "# 훈련, 검증, 테스트 데이터의 회귀 타겟을 날짜별로 출력\n",
    "ax.plot(y_train_date, y_train_regression * y_normalizer, label=\"y_train_regression\", linewidth=2)\n",
    "ax.plot(y_validation_date, y_validation_regression * y_normalizer, label=\"y_validation\", linewidth=2)\n",
    "ax.plot(y_test_date, y_test_regression * y_normalizer, label=\"y_test\", linewidth=2)\n",
    "\n",
    "# y축 레이블 설정\n",
    "ax.set_ylabel('Bitcoin [KRW]', fontsize=14)\n",
    "\n",
    "# x축 눈금 설정 (200 단위로 출력)\n",
    "ax.set_xticks(ax.get_xticks()[::200])\n",
    "\n",
    "# y축 숫자가 평범한 포맷으로 출력되도록 설정\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "# x축 레이블 회전\n",
    "plt.xticks(rotation=25)\n",
    "\n",
    "# 범례 설정\n",
    "ax.legend(loc='upper left', fontsize=16)\n",
    "\n",
    "# 그래프 출력\n",
    "plt.show()"
   ],
   "id": "efbcaca2ff8979a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# G 고찰내용\n",
    "1. DF로 데이터를 다룰 수 있다.\n",
    "2. PyTorch 텐서 처리: 데이터를 PyTorch 텐서로 변환하고 이를 모델에 입력할 수 있는 형식으로 준비할 수 있다.\n",
    "3. 데이터셋 시각화: 훈련, 검증, 테스트 데이터를 시각적으로 표현할 수 있다.\n"
   ],
   "id": "539be5037a03ce6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "import scipy.io.wavfile as wavfile\n",
    "#경로수정\n",
    "audio_1_path = os.path.join(os.path.pardir, \"_00_data\", \"f_audio-chirp\", \"1-100038-A-14.wav\")\n",
    "audio_2_path = os.path.join(os.path.pardir, \"_00_data\", \"f_audio-chirp\", \"1-100210-A-36.wav\")\n",
    "\n",
    "freq_1, waveform_arr_1 = wavfile.read(audio_1_path) # 첫 번째 오디오 파일 읽기\n",
    "print(freq_1)   # 샘플링 주파수 출력\n",
    "print(type(waveform_arr_1)) # 오디오 데이터의 타입 확인 (numpy 배열)\n",
    "print(len(waveform_arr_1))  # 오디오 데이터의 길이 출력 (샘플 수)\n",
    "print(waveform_arr_1)       # 오디오 데이터 값 출력\n",
    "\n",
    "freq_2, waveform_arr_2 = wavfile.read(audio_2_path)\n",
    "\n",
    "# 두 개의 오디오 데이터를 하나의 텐서로 결합 (형상: [2, 1, 220,500])\n",
    "# 2개의 오디오, 1채널, 각 오디오 데이터는 220,500 샘플\n",
    "waveform = torch.empty(2, 1, 220_500)\n",
    "# 첫 번째 오디오 데이터를 텐서로 변환 후 저장\n",
    "waveform[0, 0] = torch.from_numpy(waveform_arr_1).float()\n",
    "waveform[1, 0] = torch.from_numpy(waveform_arr_2).float()\n",
    "\n",
    "print(waveform.shape)   # 텐서 크기 출력 (형상: [2, 1, 220500])\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "_, _, sp_arr_1 = signal.spectrogram(waveform_arr_1, freq_1) # 첫 번째 오디오 데이터에 대한 스펙트로그램 계산 (주파수 축, 시간 축, 스펙트럼 값 반환)\n",
    "_, _, sp_arr_2 = signal.spectrogram(waveform_arr_2, freq_2)\n",
    "\n",
    "# 스펙트로그램을 텐서로 변환\n",
    "sp_1 = torch.from_numpy(sp_arr_1)\n",
    "sp_2 = torch.from_numpy(sp_arr_2)\n",
    "print(sp_1.shape)\n",
    "print(sp_2.shape)\n",
    "\n",
    "# 왼쪽 채널과 오른쪽 채널을 의미하는 텐서로 저장 (스테레오 형태)\n",
    "sp_left_t = torch.from_numpy(sp_arr_1)\n",
    "sp_right_t = torch.from_numpy(sp_arr_2)\n",
    "print(sp_left_t.shape)\n",
    "print(sp_right_t.shape)\n",
    "\n",
    "# 두 채널을 하나의 텐서로 스택 (0번째 축에 추가), 차원 확장\n",
    "sp_t = torch.stack((sp_left_t, sp_right_t), dim=0).unsqueeze(dim=0)\n",
    "print(sp_t.shape)\n"
   ],
   "id": "64f7b9eb20da9d13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# H 고찰내용\n",
    "1. 오디오 데이터의 로드와 처리\n",
    "* scipy.io.wavfile.read()함수로 오디오 데이터를 불러올 수 있다.\n",
    "* 오디오 파일은 일반적으로 numpy 배열 형태로 로드된다.\n",
    "2. PyTorch에서 오디오 데이터를 다루는 방식\n",
    "* 오디오 데이터를 torch.from_numpy()를 사용하여 numpy 배열을 텐서로 변환, .float()를 통해 실수형(float) 데이터 타입으로 변환한다.\n",
    "3. scipy.signal.spectrogram() 함수를 사용하여 오디오 데이터를 주파수 및 시간 축으로 변환하여 스펙트로그램을 계산할 수 있다.\n",
    "4. torch.from_numpy()를 사용해 스펙트로그램을 텐서로 변환한 후, torch.stack()을 이용해 두 스펙트로그램을 하나의 텐서로 결합할 수 있다."
   ],
   "id": "a4b3b31a7bb9efc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pip install imageio[ffmpeg]\n",
    "import torch\n",
    "import os\n",
    "import imageio\n",
    "#경로수정\n",
    "video_path = os.path.join(os.path.pardir, \"_00_data\", \"g_video-cockatoo\", \"cockatoo.mp4\")\n",
    "\n",
    "# imageio 라이브러리를 통해 비디오 파일을 읽기 위한 reader 객체 생성\n",
    "reader = imageio.get_reader(video_path)\n",
    "print(type(reader))\n",
    "# 비디오 파일의 메타데이터 출력 (프레임 크기, 프레임 수, 비디오 포맷 등)\n",
    "meta = reader.get_meta_data()\n",
    "print(meta)\n",
    "\n",
    "# 비디오 프레임을 하나씩 순회하며 처리\n",
    "for i, frame in enumerate(reader):\n",
    "    # 각 프레임을 numpy 배열로부터 torch 텐서로 변환 후 float 타입으로 변환\n",
    "    frame = torch.from_numpy(frame).float()  # frame.shape: [360, 480, 3]\n",
    "    print(i, frame.shape)   # i, torch.Size([360, 480, 3])\n",
    "\n",
    "# 비디오 데이터의 초기 정보 설정\n",
    "n_channels = 3  # 비디오 데이터의 채널 수 (RGB이므로 3채널)\n",
    "n_frames = 529  # 비디오 파일의 전체 프레임 수 (메타데이터에서 얻은 값)\n",
    "video = torch.empty(1, n_frames, n_channels, *meta['size'])  # (1, 529, 3, 480, 360)\n",
    "print(video.shape)\n",
    "\n",
    "# 다시 비디오 파일을 읽어 각 프레임을 텐서에 저장\n",
    "for i, frame in enumerate(reader):\n",
    "    # 각 프레임을 numpy 배열로부터 torch 텐서로 변환 후 float 타입으로 변환\n",
    "    frame = torch.from_numpy(frame).float()       # frame.shape: [360, 480, 3]\n",
    "    # torch 텐서의 차원을 순서를 변경하여 (C, W, H) 형태로 변환 (3채널, 480너비, 360높이)\n",
    "    frame = torch.permute(frame, dims=(2, 1, 0))  # frame.shape: [3, 480, 360]\n",
    "    # 해당 프레임을 비디오 텐서의 i번째 인덱스에 저장\n",
    "    video[0, i] = frame\n",
    "\n",
    "# 비디오 텐서의 차원 순서를 변경 (shape을 [1, 3, 529, 480, 360]으로 변환하여 PyTorch에서 사용하기 쉽게 변환)\n",
    "video = video.permute(dims=(0, 2, 1, 3, 4))\n",
    "print(video.shape)\n"
   ],
   "id": "8bc52e166fb3d85e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# I 고찰내용\n",
    "1. imageio.get_reader()를 통해 비디오 파일을 읽을 수 있다.\n",
    "2. get_meta_data()로 비디오 파일의 크기, 프레임 수 등 중요한 정보를 얻을 수 있다.\n",
    "3. 비디오 프레임을 numpy 배열에서 torch 텐서로 변환하고, 적절한 데이터 타입(float)으로 변환할 수 있다.\n",
    "4. torch.permute()로 채널, 너비, 높이의 차원을 변경해, CNN과 같은 모델에서 사용할 수 있는 형식으로 변환할 수 있다."
   ],
   "id": "936d7fb091dc9789"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "class LinearRegressionDataset(Dataset):\n",
    "  def __init__(self, N=50, m=-3, b=2, *args, **kwargs):\n",
    "    # N: 샘플 수, 기본값은 50\n",
    "    # m: 기울기 (slope), 기본값은 -3\n",
    "    # b: y절편 (offset), 기본값은 2\n",
    "    super().__init__(*args, **kwargs)\n",
    "    # N개의 2차원 무작위 입력 데이터 x를 생성합니다.\n",
    "    self.x = torch.rand(N, 2)\n",
    "    self.noise = torch.rand(N) * 0.2    # 무작위 노이즈를 추가하여 데이터가 더 현실적이게 만듭니다.\n",
    "    self.m = m\n",
    "    self.b = b\n",
    "    self.y = (torch.sum(self.x * self.m) + self.b + self.noise).unsqueeze(-1)   # y값을 생성: y = m * x + b + noise\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.x[idx], self.y[idx]\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.x), self.x.shape, self.y.shape\n",
    "    )\n",
    "    return str\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # LinearRegressionDataset 클래스를 이용하여 데이터셋을 생성합니다\n",
    "    linear_regression_dataset = LinearRegressionDataset()\n",
    "\n",
    "    print(linear_regression_dataset)    # 데이터셋 정보 출력\n",
    "\n",
    "    print(\"#\" * 50, 1)\n",
    "    # 데이터셋의 각 샘플(입력 데이터와 타겟 값)을 출력합니다.\n",
    "    for idx, sample in enumerate(linear_regression_dataset):\n",
    "        input, target = sample\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input, target))\n",
    "    # 데이터셋을 훈련, 검증, 테스트 세트로 나눕니다 (비율: 70%, 20%, 10%).\n",
    "    train_dataset, validation_dataset, test_dataset = random_split(linear_regression_dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "    print(\"#\" * 50, 2)\n",
    "\n",
    "    print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "    print(\"#\" * 50, 3)\n",
    "    \n",
    "    train_data_loader = DataLoader(\n",
    "        # 훈련 데이터셋을 DataLoader에 넣어 배치로 묶고 셔플링을 수행합니다.\n",
    "        dataset=train_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        # DataLoader를 이용해 배치 단위로 데이터를 출력합니다.\n",
    "        input, target = batch\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input, target))\n"
   ],
   "id": "fa21d4a7e013fe2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# J 고찰내용\n",
    "1. PyTorch Dataset 클래스와 커스텀 데이터셋 생성\n",
    "* Dataset 클래스를 상속받아 커스텀 데이터셋을 정의할 수 있다.\n",
    "*  __getitem__은 주어진 인덱스에 해당하는 데이터를 반환하며, __len__은 데이터셋의 총 길이를 반환한다.\n",
    "* 객체 정보를 사람이 읽기 좋은 형태로 출력할 수 있도록 __str__ 메서드를 오버라이딩할 수 있다.\n",
    "2. 데이터 생성\n",
    "* torch.rand()를 이용해 2차원 랜덤 데이터를 생성할 수 있다.\n",
    "* self.noise = torch.rand(N) * 0.2와 같은 코드를 통해 모델이 더 현실적인 데이터에 대한 일반화를 학습하도록 노이즈를 추가할 수 있다.\n",
    "* y = m * x + b 형태의 데이터를 생성하여, 선형 회귀에서 사용될 데이터셋을 직접 정의할 수 있다.\n",
    "3. DataLoader를 사용해 데이터셋을 배치 단위로 처리할 수 있다.\n",
    "4. random_split 함수를 사용해 데이터를 훈련, 검증, 테스트 세트로 나눌 수 있다.\n"
   ],
   "id": "83c2648e60886bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T14:41:38.355247Z",
     "start_time": "2024-09-21T14:41:34.241970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class DogCat2DImageDataset(Dataset):\n",
    "  def __init__(self):\n",
    "    # 이미지 전처리: 이미지를 (256, 256) 크기로 리사이즈한 후 텐서로 변환\n",
    "    self.image_transforms = transforms.Compose([\n",
    "      transforms.Resize(size=(256, 256)),  # 이미지 크기를 256x256으로 리사이즈\n",
    "      transforms.ToTensor()  # 이미지를 텐서로 변환\n",
    "    ])\n",
    "\n",
    "    # 강아지와 고양이 이미지 파일 경로 설정\n",
    "    dogs_dir = os.path.join(os.path.pardir, \"_00_data\", \"a_image-dog\")\n",
    "    cats_dir = os.path.join(os.path.pardir, \"_00_data\", \"b_image-cats\")\n",
    "\n",
    "    # 이미지 파일을 열고 리스트에 저장\n",
    "    image_lst = [\n",
    "      Image.open(os.path.join(dogs_dir, \"bobby.jpg\")),  # 강아지 이미지 (1280x720 크기)\n",
    "      Image.open(os.path.join(cats_dir, \"cat1.png\")),   # 고양이 이미지 1 (256x256 크기)\n",
    "      Image.open(os.path.join(cats_dir, \"cat2.png\")),   # 고양이 이미지 2 (256x256 크기)\n",
    "      Image.open(os.path.join(cats_dir, \"cat3.png\"))    # 고양이 이미지 3 (256x256 크기)\n",
    "    ]\n",
    "\n",
    "    # 이미지 전처리 적용 및 텐서화 후 스택하여 하나의 텐서로 결합 (형상: [4, 3, 256, 256])\n",
    "    image_lst = [self.image_transforms(img) for img in image_lst]\n",
    "    self.images = torch.stack(image_lst, dim=0)\n",
    "\n",
    "    # 이미지 레이블 설정: 0은 강아지, 1은 고양이 (형상: [4, 1])\n",
    "    self.image_labels = torch.tensor([[0], [1], [1], [1]])\n",
    "\n",
    "  # 데이터셋 크기를 반환하는 함수\n",
    "  def __len__(self):\n",
    "    return len(self.images)\n",
    "\n",
    "  # 인덱스를 기반으로 이미지와 해당 레이블을 반환하는 함수\n",
    "  def __getitem__(self, idx):\n",
    "    return self.images[idx], self.image_labels[idx]\n",
    "\n",
    "  # 데이터셋 정보(크기, 입력 형상, 레이블 형상)를 문자열로 반환하는 함수\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.images), self.images.shape, self.image_labels.shape\n",
    "    )\n",
    "    return str\n",
    "\n",
    "\n",
    "# 메인 실행 부분\n",
    "if __name__ == \"__main__\":\n",
    "  # DogCat2DImageDataset 객체 생성\n",
    "  dog_cat_2d_image_dataset = DogCat2DImageDataset()\n",
    "\n",
    "  # 데이터셋 정보 출력\n",
    "  print(dog_cat_2d_image_dataset)\n",
    "\n",
    "  print(\"#\" * 50, 1)\n",
    "\n",
    "  # 데이터셋 내의 각 샘플에 대해 입력 데이터와 레이블 출력\n",
    "  for idx, sample in enumerate(dog_cat_2d_image_dataset):\n",
    "    input, target = sample\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input.shape, target))\n",
    "\n",
    "  # 데이터셋을 70%는 학습 데이터, 30%는 테스트 데이터로 분할\n",
    "  train_dataset, test_dataset = random_split(dog_cat_2d_image_dataset, [0.7, 0.3])\n",
    "\n",
    "  print(\"#\" * 50, 2)\n",
    "\n",
    "  # 학습 데이터셋과 테스트 데이터셋의 크기 출력\n",
    "  print(len(train_dataset), len(test_dataset))\n",
    "\n",
    "  print(\"#\" * 50, 3)\n",
    "\n",
    "  # DataLoader를 사용하여 학습 데이터셋을 배치 크기 2로 로드\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=2,  # 배치 크기 2\n",
    "    shuffle=True   # 데이터를 섞어서 로드\n",
    "  )\n",
    "\n",
    "  # 각 배치에 대해 입력 데이터와 레이블 출력\n",
    "  for idx, batch in enumerate(train_data_loader):\n",
    "    input, target = batch\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input.shape, target))\n"
   ],
   "id": "e713b3ed62d3a04c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 4, Input Shape: torch.Size([4, 3, 256, 256]), Target Shape: torch.Size([4, 1])\n",
      "################################################## 1\n",
      "0 - torch.Size([3, 256, 256]): tensor([0])\n",
      "1 - torch.Size([3, 256, 256]): tensor([1])\n",
      "2 - torch.Size([3, 256, 256]): tensor([1])\n",
      "3 - torch.Size([3, 256, 256]): tensor([1])\n",
      "################################################## 2\n",
      "3 1\n",
      "################################################## 3\n",
      "0 - torch.Size([2, 3, 256, 256]): tensor([[1],\n",
      "        [1]])\n",
      "1 - torch.Size([1, 3, 256, 256]): tensor([[1]])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# K 고찰내용\n",
    "1. torch패키지를 사용할 시 다른 패키지와 버전 충돌이 일어나지 않도록 설치해야함.\n"
   ],
   "id": "b8db99162f692db4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # 데이터 파일 경로 설정\n",
    "        wine_path = os.path.join(os.path.pardir, \"_00_data\", \"d_tabular-wine\", \"winequality-white.csv\") # 경로수정\n",
    "        \n",
    "        # CSV 파일을 NumPy 배열로 로드\n",
    "        wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\", skiprows=1)\n",
    "        \n",
    "        # NumPy 배열을 PyTorch 텐서로 변환\n",
    "        wineq = torch.from_numpy(wineq_numpy)\n",
    "\n",
    "        # 데이터와 타겟 분리\n",
    "        data = wineq[:, :-1]  # 마지막 열을 제외한 모든 열을 데이터로 사용\n",
    "        target = wineq[:, -1].long()  # 마지막 열을 타겟으로 사용 (정수형으로 변환)\n",
    "        \n",
    "        # 데이터 정규화 (평균과 분산을 사용)\n",
    "        data_mean = torch.mean(data, dim=0)\n",
    "        data_var = torch.var(data, dim=0)\n",
    "        self.data = (data - data_mean) / torch.sqrt(data_var)\n",
    "        \n",
    "        # 타겟을 원-핫 인코딩으로 변환\n",
    "        eye_matrix = torch.eye(10)  # 10개의 클래스에 대한 단위 행렬 생성\n",
    "        self.target = eye_matrix[target]  # 타겟 값에 해당하는 행을 선택하여 원-핫 인코딩 생성\n",
    "\n",
    "        # 데이터와 타겟의 길이가 같은지 확인\n",
    "        assert len(self.data) == len(self.target)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 크기 반환\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스에 대한 데이터와 타겟을 반환\n",
    "        wine_feature = self.data[idx]\n",
    "        wine_target = self.target[idx]\n",
    "        return wine_feature, wine_target\n",
    "\n",
    "    def __str__(self):\n",
    "        # 데이터셋의 크기와 입력 및 타겟의 형상 정보 반환\n",
    "        str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "            len(self.data), self.data.shape, self.target.shape\n",
    "        )\n",
    "        return str\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # WineDataset 객체 생성\n",
    "    wine_dataset = WineDataset()\n",
    "\n",
    "    # 데이터셋 정보 출력\n",
    "    print(wine_dataset)\n",
    "\n",
    "    print(\"#\" * 50, 1)\n",
    "\n",
    "    # 데이터셋 내의 각 샘플에 대해 입력 데이터와 타겟 출력\n",
    "    for idx, sample in enumerate(wine_dataset):\n",
    "        input, target = sample\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "    # 데이터셋을 70%는 학습 데이터, 20%는 검증 데이터, 10%는 테스트 데이터로 분할\n",
    "    train_dataset, validation_dataset, test_dataset = random_split(wine_dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "    print(\"#\" * 50, 2)\n",
    "\n",
    "    # 학습 데이터셋, 검증 데이터셋, 테스트 데이터셋의 크기 출력\n",
    "    print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "    print(\"#\" * 50, 3)\n",
    "\n",
    "    # DataLoader를 사용하여 학습 데이터셋을 배치 크기 32로 로드\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=32,  # 배치 크기 32\n",
    "        shuffle=True,  # 데이터를 섞어서 로드\n",
    "        drop_last=True  # 배치 크기가 데이터셋의 크기로 나누어떨어지지 않는 경우 마지막 배치를 버림\n",
    "    )\n",
    "\n",
    "    # 각 배치에 대해 입력 데이터와 타겟 출력\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        input, target = batch\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))"
   ],
   "id": "b95f4b23301d56f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# I 고찰내용\n",
    "1. PyTorch의 Dataset 클래스를 상속하여 데이터셋을 정의할 수 있다.\n",
    "2. 배치 크기를 설정하고, 데이터를 섞어서(shuffle=True) 학습하는 과정에서 데이터의 마지막 배치를 버리는 옵션(drop_last=True)을 사용할 수 있다."
   ],
   "id": "a7898a6867b800f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class CaliforniaHousingDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # scikit-learn의 California housing 데이터셋을 로드\n",
    "        from sklearn.datasets import fetch_california_housing\n",
    "        housing = fetch_california_housing()\n",
    "\n",
    "        # 데이터 정규화: 평균과 분산을 사용하여 데이터 정규화\n",
    "        data_mean = np.mean(housing.data, axis=0)  # 각 특성의 평균\n",
    "        data_var = np.var(housing.data, axis=0)    # 각 특성의 분산\n",
    "        # 데이터 정규화 (평균 0, 분산 1로 변환)\n",
    "        self.data = torch.tensor((housing.data - data_mean) / np.sqrt(data_var), dtype=torch.float32)\n",
    "        \n",
    "        # 타겟 변수 변환: 텐서로 변환하고 차원을 추가 (모델 학습을 위해 2D 텐서로 변환)\n",
    "        self.target = torch.tensor(housing.target, dtype=torch.float32).unsqueeze(dim=-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 크기 반환\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스에 대해 데이터와 타겟을 반환\n",
    "        sample_data = self.data[idx]\n",
    "        sample_target = self.target[idx]\n",
    "        return sample_data, sample_target\n",
    "\n",
    "    def __str__(self):\n",
    "        # 데이터셋의 크기와 입력 및 타겟의 형상 정보를 문자열로 반환\n",
    "        return \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "            len(self.data), self.data.shape, self.target.shape\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # CaliforniaHousingDataset 객체 생성\n",
    "    california_housing_dataset = CaliforniaHousingDataset()\n",
    "\n",
    "    # 데이터셋 정보 출력\n",
    "    print(california_housing_dataset)\n",
    "\n",
    "    print(\"#\" * 50, 1)\n",
    "\n",
    "    # 데이터셋 내의 각 샘플에 대해 입력 데이터와 타겟 출력\n",
    "    for idx, sample in enumerate(california_housing_dataset):\n",
    "        input, target = sample\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "    # 데이터셋을 70%는 학습 데이터, 20%는 검증 데이터, 10%는 테스트 데이터로 분할\n",
    "    train_dataset, validation_dataset, test_dataset = random_split(california_housing_dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "    print(\"#\" * 50, 2)\n",
    "\n",
    "    # 학습 데이터셋, 검증 데이터셋, 테스트 데이터셋의 크기 출력\n",
    "    print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "    print(\"#\" * 50, 3)\n",
    "\n",
    "    # DataLoader를 사용하여 학습 데이터셋을 배치 크기 32로 로드\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=32,  # 배치 크기 32\n",
    "        shuffle=True,  # 데이터를 섞어서 로드\n",
    "        drop_last=True  # 배치 크기가 데이터셋의 크기로 나누어떨어지지 않는 경우 마지막 배치를 버림\n",
    "    )\n",
    "\n",
    "    # 각 배치에 대해 입력 데이터와 타겟 출력\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        input, target = batch\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))"
   ],
   "id": "e575196a2c70c85b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# M 고찰내용\n",
    "1. 데이터 셋을 정의하고 정규화 후 배치처리하는 모델 학습 준비의 과정을 알 수 있다."
   ],
   "id": "e7bfe0e3deffdf16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "BASE_PATH = str(Path(\"Deep_HW\").resolve().parent.parent)  \n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "# BikesDataset 클래스 정의\n",
    "class BikesDataset(Dataset):\n",
    "    def __init__(self, train=True, test_days=1):\n",
    "        self.train = train  # 학습용 데이터셋인지 테스트용 데이터셋인지 구분하는 플래그\n",
    "        self.test_days = test_days  # 테스트 데이터로 사용할 일수\n",
    "\n",
    "        # 데이터셋 파일 경로 설정\n",
    "        bikes_path = os.path.join(BASE_PATH, \"_00_data\", \"e_time-series-bike-sharing-dataset\", \"hour-fixed.csv\")\n",
    "\n",
    "        # CSV 파일을 NumPy 배열로 불러오기 (converters를 사용하여 날짜 컬럼에서 일(day)만 추출)\n",
    "        bikes_numpy = np.loadtxt(\n",
    "            fname=bikes_path, dtype=np.float32, delimiter=\",\", skiprows=1,\n",
    "            converters={\n",
    "                1: lambda x: float(x[8:10])  # 2011-01-07 --> 07 --> 7\n",
    "            }\n",
    "        )\n",
    "        bikes = torch.from_numpy(bikes_numpy)  # NumPy 배열을 PyTorch 텐서로 변환\n",
    "\n",
    "        # 데이터를 일(day) 단위로 나누기 (24시간씩 그룹화)\n",
    "        daily_bikes = bikes.view(-1, 24, bikes.shape[1])  # daily_bikes.shape: torch.Size([730, 24, 17])\n",
    "        self.daily_bikes_target = daily_bikes[:, :, -1].unsqueeze(dim=-1)  # 목표(target) 값 설정\n",
    "\n",
    "        self.daily_bikes_data = daily_bikes[:, :, :-1]  # 마지막 열을 제외한 나머지를 입력 데이터로 사용\n",
    "        eye_matrix = torch.eye(4)  # 날씨 데이터를 원-핫 인코딩할 때 사용할 단위 행렬 생성\n",
    "\n",
    "        # 날씨 정보를 원-핫 인코딩하여 입력 데이터에 추가\n",
    "        day_data_torch_list = []\n",
    "        for daily_idx in range(self.daily_bikes_data.shape[0]):  # range(730)\n",
    "            day = self.daily_bikes_data[daily_idx]  # day.shape: [24, 17]\n",
    "            weather_onehot = eye_matrix[day[:, 9].long() - 1]  # 날씨 데이터 원-핫 인코딩\n",
    "            day_data_torch = torch.cat(tensors=(day, weather_onehot), dim=1)  # 입력 데이터와 원-핫 인코딩된 데이터를 병합\n",
    "            day_data_torch_list.append(day_data_torch)\n",
    "\n",
    "        self.daily_bikes_data = torch.stack(day_data_torch_list, dim=0)  # 리스트를 텐서로 변환\n",
    "\n",
    "        # 데이터를 분리하여 특정 열을 제외\n",
    "        self.daily_bikes_data = torch.cat(\n",
    "            [self.daily_bikes_data[:, :, :9], self.daily_bikes_data[:, :, 10:]], dim=2\n",
    "        )\n",
    "\n",
    "        total_length = len(self.daily_bikes_data)  # 전체 데이터 길이\n",
    "        self.train_bikes_data = self.daily_bikes_data[:total_length - test_days]  # 학습 데이터\n",
    "        self.train_bikes_targets = self.daily_bikes_target[:total_length - test_days]  # 학습 데이터의 목표값\n",
    "        train_temperatures = self.train_bikes_data[:, :, 9]  # 학습 데이터에서 온도 정보 추출\n",
    "        train_temperatures_mean = torch.mean(train_temperatures)  # 온도의 평균 계산\n",
    "        train_temperatures_std = torch.std(train_temperatures)  # 온도의 표준편차 계산\n",
    "\n",
    "        # 학습 데이터의 온도 정보를 정규화\n",
    "        self.train_bikes_data[:, :, 9] = \\\n",
    "            (self.train_bikes_data[:, :, 9] - train_temperatures_mean) / train_temperatures_std\n",
    "\n",
    "        # 테스트 데이터가 필요한 경우, 테스트 데이터 설정 및 정규화\n",
    "        if not train:\n",
    "            self.test_bikes_data = self.daily_bikes_data[-test_days:]  # 테스트 데이터\n",
    "            self.test_bikes_targets = self.daily_bikes_target[-test_days:]  # 테스트 데이터의 목표값\n",
    "            self.test_bikes_data[:, :, 9] = \\\n",
    "                (self.test_bikes_data[:, :, 9] - train_temperatures_mean) / train_temperatures_std  # 온도 정규화\n",
    "\n",
    "    # 데이터셋의 크기 반환\n",
    "    def __len__(self):\n",
    "        return len(self.train_bikes_data) if self.train is True else len(self.test_bikes_data)\n",
    "\n",
    "    # 주어진 인덱스에 해당하는 입력 데이터와 목표값 반환\n",
    "    def __getitem__(self, idx):\n",
    "        bike_feature = self.train_bikes_data[idx] if self.train is True else self.test_bikes_data[idx]\n",
    "        bike_target = self.train_bikes_targets[idx] if self.train is True else self.test_bikes_targets[idx]\n",
    "        return bike_feature, bike_target\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 학습 데이터셋 생성\n",
    "    train_bikes_dataset = BikesDataset(train=True, test_days=1)\n",
    "    print(train_bikes_dataset)\n",
    "\n",
    "    print(\"#\" * 50, 1)\n",
    "\n",
    "    # 학습 데이터셋을 80% 학습 데이터와 20% 검증 데이터로 분할\n",
    "    train_dataset, validation_dataset = random_split(train_bikes_dataset, [0.8, 0.2])\n",
    "\n",
    "    # 학습 데이터셋 출력\n",
    "    print(\"[TRAIN]\")\n",
    "    for idx, sample in enumerate(train_dataset):\n",
    "        input, target = sample\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "    # 학습 데이터를 DataLoader로 배치 단위로 로드\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "    # 배치 단위로 데이터 출력\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        input, target = batch\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "    print(\"#\" * 50, 2)\n",
    "\n",
    "    # 검증 데이터셋 출력\n",
    "    print(\"[VALIDATION]\")\n",
    "    for idx, sample in enumerate(validation_dataset):\n",
    "        input, target = sample\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "    # 검증 데이터를 DataLoader로 로드\n",
    "    validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=32)\n",
    "\n",
    "    # 배치 단위로 검증 데이터 출력\n",
    "    for idx, batch in enumerate(validation_data_loader):\n",
    "        input, target = batch\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "    print(\"#\" * 50, 3)\n",
    "\n",
    "    # 테스트 데이터셋 생성\n",
    "    test_dataset = BikesDataset(train=False, test_days=1)\n",
    "    print(test_dataset)\n",
    "\n",
    "    # 테스트 데이터 출력\n",
    "    print(\"[TEST]\")\n",
    "    for idx, sample in enumerate(test_dataset):\n",
    "        input, target = sample\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "    # 테스트 데이터를 DataLoader로 로드\n",
    "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "    # 테스트 데이터 출력\n",
    "    for idx, batch in enumerate(test_data_loader):\n",
    "        input, target = batch\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n"
   ],
   "id": "43bc78f8092d7c62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4eca465275a80255"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# N 고찰 내용\n",
    "1. PyTorch Dataset 및 DataLoader 활용할 수 있다.\n",
    "2. np.loadtxt의 converters 인자를 사용하여, 날짜 데이터를 처리할 때 2011-01-07 형식에서 07만 추출하는 커스텀 변환기를 적용할 수 있다.\n",
    "3. 데이터를 재구성할 때 사용하는 view 메서드는 특정 차원으로 데이터를 변환하는 데 유용하다."
   ],
   "id": "c6c4fbc34ccf2149"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# 프로젝트 디렉토리 경로 설정\n",
    "BASE_PATH = str(Path(\"Deep_HW\").resolve().parent.parent) \n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "# 자전거 대여 데이터셋을 위한 Dataset 클래스 정의\n",
    "class HourlyBikesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # 입력 데이터(X)와 타겟 데이터(y)를 받아서 초기화\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        # X와 y의 크기가 같은지 확인\n",
    "        assert len(self.X) == len(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 크기 반환\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스(idx)에 해당하는 입력 데이터와 타겟 데이터 반환\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        return X, y\n",
    "\n",
    "    def __str__(self):\n",
    "        # 데이터셋 정보(크기, 입력 데이터 모양, 타겟 데이터 모양)를 문자열로 반환\n",
    "        str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "            len(self.X), self.X.shape, self.y.shape\n",
    "        )\n",
    "        return str\n",
    "\n",
    "# 자전거 대여 데이터를 불러오고 학습, 검증, 테스트용 데이터셋을 만드는 함수 정의\n",
    "def get_hourly_bikes_data(sequence_size=24, validation_size=96, test_size=24, y_normalizer=100):\n",
    "    # 자전거 대여 데이터 파일 경로\n",
    "    bikes_path = os.path.join(BASE_PATH, \"_00_data\", \"e_time-series-bike-sharing-dataset\", \"hour-fixed.csv\")\n",
    "\n",
    "    # CSV 파일을 불러와 NumPy 배열로 변환 (1번 컬럼을 날짜로 변환하여 특정 형식에 맞춤)\n",
    "    bikes_numpy = np.loadtxt(\n",
    "        fname=bikes_path, dtype=np.float32, delimiter=\",\", skiprows=1,\n",
    "        converters={\n",
    "            1: lambda x: float(x[8:10])  # 2011-01-07 --> 07 --> 7\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # NumPy 배열을 PyTorch 텐서로 변환하고, 마지막 열('cnt' 열)을 타겟 데이터로 분리\n",
    "    bikes_data = torch.from_numpy(bikes_numpy).to(torch.float) # >>> torch.Size([17520, 17])\n",
    "    bikes_target = bikes_data[:, -1].unsqueeze(dim=-1)  # 'cnt' 열을 타겟으로 사용\n",
    "    bikes_data = bikes_data[:, :-1]  # 마지막 열을 제외한 나머지를 입력 데이터로 사용\n",
    "\n",
    "    # 날씨 데이터를 원-핫 인코딩하기 위한 단위 행렬\n",
    "    eye_matrix = torch.eye(4)\n",
    "\n",
    "    # 날씨 정보를 원-핫 인코딩하여 데이터를 변환\n",
    "    data_torch_list = []\n",
    "    for idx in range(bikes_data.shape[0]):\n",
    "        hour_data = bikes_data[idx]  # 각 시간 데이터\n",
    "        weather_onehot = eye_matrix[hour_data[9].long() - 1]  # 날씨 정보를 원-핫 인코딩\n",
    "        concat_data_torch = torch.cat(tensors=(hour_data, weather_onehot), dim=-1)  # 데이터와 원-핫 인코딩 데이터를 병합\n",
    "        data_torch_list.append(concat_data_torch)\n",
    "\n",
    "    # 리스트를 텐서로 변환하여 새로운 데이터셋으로 만듦\n",
    "    bikes_data = torch.stack(data_torch_list, dim=0)\n",
    "    # 불필요한 열(시간 및 원래 날씨 열)을 제거하여 최종 입력 데이터 구성\n",
    "    bikes_data = torch.cat([bikes_data[:, 1:9], bikes_data[:, 10:]], dim=-1)\n",
    "    print(bikes_data.shape, \"!!!\")  # >>> torch.Size([17520, 18])\n",
    "\n",
    "    # 학습, 검증, 테스트 데이터셋의 크기 계산\n",
    "    data_size = len(bikes_data) - sequence_size\n",
    "    train_size = data_size - (validation_size + test_size)\n",
    "\n",
    "    # 학습 데이터셋 구성\n",
    "    row_cursor = 0\n",
    "    X_train_list = []\n",
    "    y_train_regression_list = []\n",
    "    for idx in range(0, train_size):\n",
    "        sequence_data = bikes_data[idx: idx + sequence_size]  # 입력 데이터 시퀀스\n",
    "        sequence_target = bikes_target[idx + sequence_size - 1]  # 해당 시퀀스의 타겟 값\n",
    "        X_train_list.append(sequence_data)\n",
    "        y_train_regression_list.append(sequence_target)\n",
    "        row_cursor += 1\n",
    "\n",
    "    # 학습 데이터 텐서로 변환\n",
    "    X_train = torch.stack(X_train_list, dim=0).to(torch.float)\n",
    "    y_train_regression = torch.tensor(y_train_regression_list, dtype=torch.float32) / y_normalizer\n",
    "\n",
    "    # 학습 데이터 정규화 (평균과 표준편차를 사용)\n",
    "    m = X_train.mean(dim=0, keepdim=True)\n",
    "    s = X_train.std(dim=0, keepdim=True)\n",
    "    X_train = (X_train - m) / s\n",
    "\n",
    "    # 검증 데이터셋 구성\n",
    "    X_validation_list = []\n",
    "    y_validation_regression_list = []\n",
    "    for idx in range(row_cursor, row_cursor + validation_size):\n",
    "        sequence_data = bikes_data[idx: idx + sequence_size]\n",
    "        sequence_target = bikes_target[idx + sequence_size - 1]\n",
    "        X_validation_list.append(sequence_data)\n",
    "        y_validation_regression_list.append(sequence_target)\n",
    "        row_cursor += 1\n",
    "\n",
    "    # 검증 데이터 텐서로 변환 및 정규화\n",
    "    X_validation = torch.stack(X_validation_list, dim=0).to(torch.float)\n",
    "    y_validation_regression = torch.tensor(y_validation_regression_list, dtype=torch.float32) / y_normalizer\n",
    "    X_validation -= m\n",
    "    X_validation /= s\n",
    "\n",
    "    # 테스트 데이터셋 구성\n",
    "    X_test_list = []\n",
    "    y_test_regression_list = []\n",
    "    for idx in range(row_cursor, row_cursor + test_size):\n",
    "        sequence_data = bikes_data[idx: idx + sequence_size]\n",
    "        sequence_target = bikes_target[idx + sequence_size - 1]\n",
    "        X_test_list.append(sequence_data)\n",
    "        y_test_regression_list.append(sequence_target)\n",
    "        row_cursor += 1\n",
    "\n",
    "    # 테스트 데이터 텐서로 변환 및 정규화\n",
    "    X_test = torch.stack(X_test_list, dim=0).to(torch.float)\n",
    "    y_test_regression = torch.tensor(y_test_regression_list, dtype=torch.float32) / y_normalizer\n",
    "    X_test -= m\n",
    "    X_test /= s\n",
    "\n",
    "    # 학습, 검증, 테스트 데이터셋 반환\n",
    "    return (\n",
    "        X_train, X_validation, X_test,\n",
    "        y_train_regression, y_validation_regression, y_test_regression\n",
    "    )\n",
    "\n",
    "\n",
    "# 메인 코드: 데이터셋을 불러와서 DataLoader로 구성하고 출력\n",
    "if __name__ == \"__main__\":\n",
    "    X_train, X_validation, X_test, y_train, y_validation, y_test = get_hourly_bikes_data(\n",
    "        sequence_size=24, validation_size=96, test_size=24, y_normalizer=100\n",
    "    )\n",
    "\n",
    "    print(\"Train: {0}, Validation: {1}, Test: {2}\".format(len(X_train), len(X_validation), len(X_test)))\n",
    "\n",
    "    # 학습, 검증, 테스트 데이터셋을 각각 HourlyBikesDataset으로 래핑\n",
    "    train_hourly_bikes_dataset = HourlyBikesDataset(X=X_train, y=y_train)\n",
    "    validation_hourly_bikes_dataset = HourlyBikesDataset(X=X_validation, y=y_validation)\n",
    "    test_houly_bikes_dataset = HourlyBikesDataset(X=X_test, y=y_test)\n",
    "\n",
    "    # 학습 데이터셋을 DataLoader로 로드하여 배치 단위로 처리\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=train_hourly_bikes_dataset, batch_size=32, shuffle=True, drop_last=True\n",
    "    )\n",
    "\n",
    "    # 데이터 배치 출력 코드\n",
    "    # for idx, batch in enumerate(train_data_loader):\n",
    "    #     input, target = batch\n",
    "    #     print(\"{0} - {1}: {2}, {3}\".format(idx, input.shape, target.shape, target))\n"
   ],
   "id": "6529eb5f15319a13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# O 고찰내용\n",
    "#### dataset을 활용해서 생기는 이점\n",
    "1. 효율적인 메모리 사용\n",
    "2. 학습 성능 향상\n",
    "3. 셔플링(Shuffling)\n",
    "4. 드롭 아웃 기능\n",
    "5. 유연한 배치 크기 설정\n",
    "6. 병렬 데이터 로드"
   ],
   "id": "c3f9dade48556f69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# https://towardsdatascience.com/cryptocurrency-price-prediction-using-deep-learning-70cfca50dd3a\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "BASE_PATH = str(Path(\"Deep_HW\").resolve().parent.parent) \n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "\n",
    "class CryptoCurrencyDataset(Dataset):\n",
    "  def __init__(self, X, y, is_regression=True):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "    assert len(self.X) == len(self.y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    X = self.X[idx]\n",
    "    y = self.y[idx]\n",
    "    return X, y\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.X), self.X.shape, self.y.shape\n",
    "    )\n",
    "    return str\n",
    "\n",
    "\n",
    "def get_cryptocurrency_data(\n",
    "    sequence_size=10, validation_size=100, test_size=10, target_column='Close', y_normalizer=1.0e7, is_regression=True\n",
    "):\n",
    "  btc_krw_path = os.path.join(BASE_PATH, \"_00_data\", \"k_cryptocurrency\", \"BTC_KRW.csv\")\n",
    "  df = pd.read_csv(btc_krw_path)\n",
    "  row_size = len(df)\n",
    "  # ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "  date_list = df['Date']\n",
    "\n",
    "  df = df.drop(columns=['Date'])\n",
    "\n",
    "  data_size = row_size - sequence_size\n",
    "  train_size = data_size - (validation_size + test_size)\n",
    "  #################################################################################################\n",
    "\n",
    "  row_cursor = 0\n",
    "\n",
    "  X_train_list = []\n",
    "  y_train_regression_list = []\n",
    "  y_train_classification_list = []\n",
    "  y_train_date = []\n",
    "  for idx in range(0, train_size):\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 5)\n",
    "    X_train_list.append(torch.from_numpy(sequence_data))\n",
    "    y_train_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "    y_train_classification_list.append(\n",
    "      1 if df.iloc[idx + sequence_size][target_column] >= df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "    )\n",
    "    y_train_date.append(date_list[idx + sequence_size])\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_train = torch.stack(X_train_list, dim=0).to(torch.float)\n",
    "  y_train_regression = torch.tensor(y_train_regression_list, dtype=torch.float32) / y_normalizer\n",
    "  y_train_classification = torch.tensor(y_train_classification_list, dtype=torch.int64)\n",
    "\n",
    "  m = X_train.mean(dim=0, keepdim=True)\n",
    "  s = X_train.std(dim=0, keepdim=True)\n",
    "  X_train = (X_train - m) / s\n",
    "\n",
    "  #################################################################################################\n",
    "\n",
    "  X_validation_list = []\n",
    "  y_validation_regression_list = []\n",
    "  y_validation_classification_list = []\n",
    "  y_validation_date = []\n",
    "  for idx in range(row_cursor, row_cursor + validation_size):\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 5)\n",
    "    X_validation_list.append(torch.from_numpy(sequence_data))\n",
    "    y_validation_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "    y_validation_classification_list.append(\n",
    "      1 if df.iloc[idx + sequence_size][target_column] >= df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "    )\n",
    "    y_validation_date.append(date_list[idx + sequence_size])\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_validation = torch.stack(X_validation_list, dim=0).to(torch.float)\n",
    "  y_validation_regression = torch.tensor(y_validation_regression_list, dtype=torch.float32) / y_normalizer\n",
    "  y_validation_classification = torch.tensor(y_validation_classification_list, dtype=torch.int64)\n",
    "\n",
    "  X_validation = (X_validation - m) / s\n",
    "  #################################################################################################\n",
    "\n",
    "  X_test_list = []\n",
    "  y_test_regression_list = []\n",
    "  y_test_classification_list = []\n",
    "  y_test_date = []\n",
    "  for idx in range(row_cursor, row_cursor + test_size):\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 5)\n",
    "    X_test_list.append(torch.from_numpy(sequence_data))\n",
    "    y_test_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "    y_test_classification_list.append(\n",
    "      1 if df.iloc[idx + sequence_size][target_column] > df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "    )\n",
    "    y_test_date.append(date_list[idx + sequence_size])\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_test = torch.stack(X_test_list, dim=0).to(torch.float)\n",
    "  y_test_regression = torch.tensor(y_test_regression_list, dtype=torch.float32) / y_normalizer\n",
    "  y_test_classification = torch.tensor(y_test_classification_list, dtype=torch.int64)\n",
    "\n",
    "  X_test = (X_test - m) / s\n",
    "\n",
    "  if is_regression:\n",
    "    return (\n",
    "      X_train, X_validation, X_test,\n",
    "      y_train_regression, y_validation_regression, y_test_regression,\n",
    "      y_train_date, y_validation_date, y_test_date\n",
    "    )\n",
    "  else:\n",
    "    return (\n",
    "      X_train, X_validation, X_test,\n",
    "      y_train_classification, y_validation_classification, y_test_classification,\n",
    "      y_train_date, y_validation_date, y_test_date\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  is_regression = False\n",
    "\n",
    "  X_train, X_validation, X_test, y_train, y_validation, y_test, y_train_date, y_validation_date, y_test_date \\\n",
    "    = get_cryptocurrency_data(\n",
    "    sequence_size=10, validation_size=100, test_size=10,\n",
    "    target_column='Close', y_normalizer=1.0e7, is_regression=is_regression\n",
    "  )\n",
    "\n",
    "  train_crypto_currency_dataset = CryptoCurrencyDataset(X=X_train, y=y_train, is_regression=is_regression)\n",
    "  validation_crypto_currency_dataset = CryptoCurrencyDataset(X=X_validation, y=y_validation, is_regression=is_regression)\n",
    "  test_crypto_currency_dataset = CryptoCurrencyDataset(X=X_test, y=y_test, is_regression=is_regression)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=train_crypto_currency_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    "  )\n",
    "\n",
    "  for idx, batch in enumerate(train_data_loader):\n",
    "    input, target = batch\n",
    "    print(\"{0} - {1}: {2}, {3}\".format(idx, input.shape, target.shape, target))\n",
    "\n"
   ],
   "id": "d4383ba834473234",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# P 고찰내용\n",
    "#### dataset을 사용하지 않은 방식과의 차이점\n",
    "1. 데이터 처리 방식의 차이\n",
    "* 기존 버전은 데이터배치나 셔플링 등을 수작업으로 처리 하지만 dataset을 활용하면 자동으로 한다.\n",
    "2. dataset을 활용하면 중복된 코드를 피할 수 있다.\n",
    "3. 코드의 가독성이 좋아진다\n",
    "# 숙제후기\n",
    "숙제 1에서 텐서를 다루는 방식을 배우고 숙제 2에서는 배운 방식을 활용해서 데이터를 정제하고 분할하고 머신러닝을 위한 준비 과정을 배웠다고 느꼇습니다. 모든 내용을 제걸로 만들지는 못 했지만 최대한 이해하고 직접 코드를 수행해보며 정규화의 과정과 각 데이터별로 적합한 형태로 변환하는 과정 등을 배워 흥미로웠습니다."
   ],
   "id": "1c161ee62d4292c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "26c3685236413ccd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
