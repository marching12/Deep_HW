{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "\n",
    "a1 = torch.tensor(1)\t\t\t     # shape: torch.Size([]), ndims(=rank): 0\n",
    "print(a1.shape, a1.ndim)\n",
    "\n",
    "a2 = torch.tensor([1])\t\t  \t     # shape: torch.Size([1]), ndims(=rank): 1\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])   # shape: torch.Size([5]), ndims(=rank): 1\n",
    "print(a3.shape, a3.ndim)\n",
    "\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])   # shape: torch.Size([5, 1]), ndims(=rank): 2\n",
    "print(a4.shape, a4.ndim)\n",
    "\n",
    "a5 = torch.tensor([                 # shape: torch.Size([3, 2]), ndims(=rank): 2\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)\n",
    "\n",
    "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]), ndims(=rank): 3\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "a8 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3]), ndims(=rank): 4\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "\n",
    "a9 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3, 1]), ndims(=rank): 5\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 5]), ndims(=rank): 2\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 1, 5]), ndims(=rank): 3\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\"\"\"\n",
    "a11 = torch.tensor([                 # ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])\n",
    "-> 각각의 차원은 같은 사이즈를 가져야함, 1차원에 데이터 4개 2차원에 1개 3차원에 2개 4차원에 3개이거나 2개이어야 함 같은 차원에 사이즈가 다른 데이터가 존재하여 요류발생\n",
    "\"\"\"\n",
    " \n",
    "#-> 수정 추가코드 \n",
    "a11 = torch.tensor([                 \n",
    "    [[[1, 2, 3], [4, 5, 6]]],\n",
    "    [[[1, 2, 3], [4, 5, 6]]],\n",
    "    [[[1, 2, 3], [4, 5, 6]]],\n",
    "    [[[1, 2, 3], [4, 5, 6]]],\n",
    "])\n",
    "\n",
    "\n",
    "# torch.Tensor class\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu')  #데이터를 직접 넣어서 초기화\n",
    "print(t1.dtype)   # >>> torch.float32\n",
    "print(t1.device)  # >>> cpu\n",
    "print(t1.requires_grad)  # >>> False (텐서의 모든 기울기를 저장)\n",
    "print(t1.size())  # torch.Size([3])\n",
    "print(t1.shape)   # torch.Size([3])\n",
    "\n",
    "#if you have gpu device\n",
    "# -> t1_cuda = t1.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# -> t1_cuda = t1.cuda()\n",
    "\n",
    "t1_cpu = t1.cpu()   #디바이스를 cpu로 만듬\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# torch.tensor function\n",
    "t2 = torch.tensor([1, 2, 3], device='cpu')\n",
    "print(t2.dtype)  # >>> torch.int64\n",
    "print(t2.device)  # >>> cpu\n",
    "print(t2.requires_grad)  # >>> False\n",
    "print(t2.size())  # torch.Size([3])\n",
    "print(t2.shape)  # torch.Size([3])\n",
    "\n",
    "# if you have gpu device\n",
    "# t2_cuda = t2.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t2_cuda = t2.cuda()\n",
    "t2_cpu = t2.cpu()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 고찰내용\n",
    "1. 괄호로 나누어진 차원을 구분할 줄 알아야 함.\n",
    "2. .Tensor -> 자료형이 float32 .tensor -> 자료형 int 64."
   ],
   "id": "4e0b1117dafa8102"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "l1 = [1, 2, 3]  #데이터 정의\n",
    "t1 = torch.Tensor(l1) #정의한 데이터로 텐서 생성\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3)\n",
    "\n",
    "l1[0] = 100 #데이터를 바꾸어도 값이 변하지 않음\n",
    "l2[0] = 100 \n",
    "l3[0] = 100\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "print(\"#\" * 100)\n",
    "\n",
    "l4 = np.array([1, 2, 3])    #numpy 데이터 생성\n",
    "t4 = torch.Tensor(l4)       #텐서 생성 (메모리 카피)\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)    #텐서 생성 (메모리 참조)\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100     #값이 변경된다.\n",
    "\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6)"
   ],
   "id": "5f4c0d70f8cbba78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# B 고찰내용\n",
    "1. 텐서는 데이터를 카피하기 때문에 데이터를 재정의해도 값이 변하지 않는다.\n",
    "2. 다만 numpy자료형을 활용하면 메모리 참조가 가능해 값을 변경할 수 있다.\n",
    "2. 텐서는 보통 데이터를 참조하지 않고 생성하기 때문에 삭제가 중요하다."
   ],
   "id": "a8ab86abc5c87a48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(5,))  # 모든 값을 1로 채운다\n",
    "t1_like = torch.ones_like(input=t1) #input_tensor와 사이즈가 같은 텐서를 1로 채워서 반환\n",
    "print(t1)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "print(t1_like)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "\n",
    "t2 = torch.zeros(size=(6,))  # 모든 값을 0으로 채운다\n",
    "t2_like = torch.zeros_like(input=t2)    #input_tenssor와 사이즈가 같은 텐서를 0으로 채워서 반환\n",
    "print(t2)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "print(t2_like)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "t3 = torch.empty(size=(4,))  # or torch.zeros(4)    #초기화 되지 않은 데이터를 사용해 메모리 절약가능 (매번 다른 데이터가 채워짐)\n",
    "t3_like = torch.empty_like(input=t3) \n",
    "print(t3)  # >>> tensor([0., 0., 0., 0.])\n",
    "print(t3_like)  # >>> tensor([0., 0., 0., 0.])\n",
    "\n",
    "t4 = torch.eye(n=3)    #사이즈가 n인 항등행렬로 텐서를 채움\n",
    "print(t4)\n"
   ],
   "id": "5a8e2bd5f7ac8a5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# C고찰 내용 \n",
    "#### (특정 차원의 텐서를 만들고 싶을 때는 명시하지 않고 만들 수 있는가?)\n",
    "1. 여러 함수로 해결 가능\n",
    "2. 1로 채우거나 0으로 채우거나\n",
    "3. 랜덤으로채우거나\n",
    "4. 항등행렬로 채우는 방법이 있다"
   ],
   "id": "7c9721f688987725"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
    "print(t1)\n",
    "\n",
    "t2 = torch.rand(size=(1, 3))\n",
    "print(t2)\n",
    "\n",
    "t3 = torch.randn(size=(1, 3))\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))\n",
    "print(t4)\n",
    "\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)\n",
    "print(t5)\n",
    "\n",
    "t6 = torch.arange(5)\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)\n"
   ],
   "id": "4c73375cea9c212c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
